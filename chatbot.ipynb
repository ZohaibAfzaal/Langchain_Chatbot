{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from PyPDF2 import PdfReader\n",
    "import pdfplumber\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate\n",
    "from docx import Document as DocxDocument  \n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from uuid import uuid4\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your documents are stored\n",
    "directory_path = \".\"\n",
    "\n",
    "# List of files with their associated tags\n",
    "files_with_tags = [\n",
    "    (\"AboutUs.pdf\", \"AboutUs\"),\n",
    "    (\"Programs.pdf\", \"Programs\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define paths\n",
    "faiss_index_path = \"faiss_index12\"\n",
    "documents = []\n",
    "\n",
    "\n",
    "if os.path.exists(faiss_index_path):\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "    document_search = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"FAISS index loaded successfully.\")\n",
    "else:\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    # Load and tag the PDF documents\n",
    "    documents = []\n",
    "    for file_name, tag in files_with_tags:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if file_name.endswith('.pdf'):\n",
    "            # Process PDF documents\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "                # Split the text into chunks\n",
    "                chunks = text_splitter.split_text(text)\n",
    "                for chunk in chunks:\n",
    "                    document = Document(page_content=chunk, metadata={\"source\": file_name, \"tag\": tag})\n",
    "                    documents.append(document)\n",
    "\n",
    "    # Download embeddings from OpenAI/ Assign unique UUIDs to the documents\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "    # Create a FAISS vector store from the documents\n",
    "    document_search = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    # Save the FAISS index locally\n",
    "    os.makedirs(faiss_index_path, exist_ok=True)  # Ensure the directory exists\n",
    "    document_search.save_local(faiss_index_path)\n",
    "    print(\"FAISS index created and saved successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use the loaded or newly created FAISS index to perform similarity searches\n",
    "docs = document_search.similarity_search(\"qux\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_template = \"\"\"\n",
    "  You are a helpful and friendly conversational assistant that provides concise and accurate answers regarding information on the TUKL Lab website at NUST.\n",
    "            Follow these guidelines:\n",
    "            - Read the context and chat history carefully before answering.\n",
    "            - Correct any spelling mistakes in the user's messages.\n",
    "            - Ask clarifying questions if more information is needed.\n",
    "            - Always maintain a polite and respectful tone.\n",
    "            - Keep your responses very relevant.\n",
    "            - Utilize the chat history and context to gather information and provide accurate answers.\n",
    "            - If the provided context is unclear or incomplete, ask for clarification to ensure your answers are relevant and helpful.\n",
    "            - Please make sense of abbreviations if any are used.\n",
    "            - current YEAR  is 2024. \n",
    "            - Please keep your answers brief and conversational, like in a real-life conversation.\n",
    "            \n",
    "\n",
    "Please respond to the following question after reading the context and chat history with respect and clarity:\n",
    "History: {history}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create PromptTemplate instance\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['question', 'context', 'history'],\n",
    "    template=demo_template\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(prompt=prompt, llm=LLM)\n",
    "\n",
    "# Initialize memory to store chat history\n",
    "memory = ConversationBufferMemory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_source_and_file(query):\n",
    "    # Extract sources (tags) and file names separately\n",
    "    sources = [tag for _, tag in files_with_tags]\n",
    "    file_names = [file_name for file_name, _ in files_with_tags]\n",
    "    \n",
    "    # Create prompts for predicting the source and file name\n",
    "    source_prompt = f\"Given the query: '{query}', predict the most appropriate source from the following: {sources}.\"\n",
    "    file_name_prompt = f\"Given the query: '{query}', predict the most appropriate file name from the following: {file_names}.\"\n",
    "    \n",
    "    # Get predictions from the LLM\n",
    "   # predicted_source = LLM(prompt=source_prompt).strip().lower()  # Predict and clean the source\n",
    "    #predicted_file_name = LLM(prompt=file_name_prompt).strip()  # Predict and clean the file name\n",
    "    predicted_source = LLM(messages=[{\"role\": \"system\", \"content\": source_prompt}]).content.strip().lower()\n",
    "    predicted_file_name = LLM(messages=[{\"role\": \"system\", \"content\": file_name_prompt}]).content.strip()\n",
    "   # print(predicted_source)\n",
    "    #print(predicted_file_name)\n",
    "    return predicted_source, predicted_file_name\n",
    "\n",
    "def get_response(input_text):\n",
    "    if input_text:\n",
    "        # Perform initial similarity search\n",
    "        docs = document_search.similarity_search(input_text,k=40)\n",
    "                \n",
    "        # Predict the source and file name based on the input_text\n",
    "        #predicted_source, predicted_file_name = predict_source_and_file(input_text)\n",
    "    \n",
    "        # Retrieve results with MMR filtering\n",
    "        #retriever = document_search.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\n",
    "        #retrieved_results = retriever.invoke(input_text, filter={\"source\": predicted_file_name})\n",
    "        \n",
    "        # Generate context from the similarity search results\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Get history from memory\n",
    "        history = memory.load_memory_variables({}).get('history', [])\n",
    "        \n",
    "        # Get the response from the QA chain using the formatted prompt\n",
    "        response = qa_chain.run({  \"context\": context,\"question\": input_text,\"history\": history  })\n",
    "        #print(response)\n",
    "        #tags = [doc.metadata.get(\"tag\") for doc in docs]\n",
    "        #print(f\"Tags: {tags}\")\n",
    "        # Combine normal and filtered responses\n",
    "       # combined_response = f\"Normal Response: {response}\\nFiltered Response: {[result.page_content for result in retrieved_results]}\"\n",
    "        \n",
    "        # Create a PromptTemplate instance for the final synthesis\n",
    "        final_template = \"\"\"\n",
    "        You are an expert assistant tasked with synthesizing responses. You have received the following two responses, and you need to generate a concise and informative final answer:\n",
    "        You have to choose best answers from the following responses and generate a final answer .You should answer based on the context and chat history.\n",
    "        {question}\n",
    "        {response}\n",
    "        {context}\n",
    "\n",
    "        Please generate a final answer based on the above information.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a PromptTemplate instance for the final synthesis\n",
    "        final_prompt = PromptTemplate(\n",
    "            input_variables=['combined_response'],\n",
    "            template=final_template\n",
    "        )\n",
    "\n",
    "        # Create a new LLMChain for the final synthesis\n",
    "        final_qa_chain = LLMChain(prompt=final_prompt, llm=LLM)\n",
    "\n",
    "        # Generate the final answer\n",
    "        final_answer = final_qa_chain.run({\"response\": response,\"context\": context,\n",
    "            \"question\": input_text})\n",
    "        \n",
    "        # Print the final answer\n",
    "        print(f\"Final Answer: {final_answer}\")\n",
    "        \n",
    "        # Update chat history\n",
    "        memory.save_context({\"input\": input_text}, {\"output\": final_answer})\n",
    "        print(\"Chat history:\", memory.load_memory_variables({}))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zohaib Afzaal\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: The current Rector of the National University of Technology (NUTECH) is Lt Gen (Retd) Moazzam Ejaz, HI (M). He has been instrumental in establishing NUTECH as a fully functional university focused on integrating academia with industry and character development. NUTECH aims to be a world-class, technology-driven research university dedicated to advancing knowledge in applied sciences, engineering, and technology, while fostering entrepreneurship and innovation. The university offers a range of undergraduate and graduate programs, as well as vocational training, to equip students with the skills needed for the modern workforce. NUTECH's mission is to contribute to Pakistan's socio-economic progress by producing skilled professionals and promoting sustainable development through strong industry partnerships.\n",
      "Chat history: {'history': \"Human: Who is rector of nutech\\nAI: The current Rector of the National University of Technology (NUTECH) is Lt Gen (Retd) Moazzam Ejaz, HI (M). He has been instrumental in establishing NUTECH as a fully functional university focused on integrating academia with industry and character development. NUTECH aims to be a world-class, technology-driven research university dedicated to advancing knowledge in applied sciences, engineering, and technology, while fostering entrepreneurship and innovation. The university offers a range of undergraduate and graduate programs, as well as vocational training, to equip students with the skills needed for the modern workforce. NUTECH's mission is to contribute to Pakistan's socio-economic progress by producing skilled professionals and promoting sustainable development through strong industry partnerships.\"}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Who is rector of nutech\"    \n",
    "get_response(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history cleared.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to clear the chat history\n",
    "def clear_chat_history():\n",
    "    memory.clear()\n",
    "    print(\"Chat history cleared.\")\n",
    "\n",
    "# Example usage: Call this function to clear the memory\n",
    "clear_chat_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
